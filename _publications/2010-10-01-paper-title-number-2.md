---
title: "Curriculum Hierarchical Knowledge Distillation for Bias-Free Survival Prediction"
collection: publications
category: conferences
permalink: /publication/2010-10-01-paper-title-number-2
excerpt: >-
  A key fairness challenge for pathology foundation models is their performance drop-off on data-sparse patient groups, despite excelling on data-rich ones. To address this, we developed a method to effectively transfer knowledge from data-rich to data-poor domains. Our approach combines hierarchical knowledge distillation and curriculum learning, using "virtual samples" to progressively transfer knowledge from well-represented cases to under-represented ones, which significantly boosts model accuracy in data-sparse environments.
venue: 'the 34th International Joint Conference on Artificial Intelligence (IJCAI 2025), TBD'
---
